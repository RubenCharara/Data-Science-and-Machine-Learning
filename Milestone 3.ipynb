{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Maximalanzeigen f√ºr Pandas (vorsichtig nutzen bei gro√üen Daten)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# --- Daten laden und vorbereiten ---\n",
    "df = pd.read_csv(\"202408-bluebikes-tripdata.csv\", sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "df['started_at'] = pd.to_datetime(df['started_at'], dayfirst=True, errors='coerce')\n",
    "df = df.dropna(subset=['started_at'])\n",
    "df['date'] = df['started_at'].dt.normalize()\n",
    "df['hour'] = df['started_at'].dt.hour.astype('int8')\n",
    "df['weekday'] = df['started_at'].dt.dayofweek.astype('int8')\n",
    "\n",
    "agg = df.groupby(['start_station_id', 'date', 'hour']).size().reset_index(name='count')\n",
    "agg['weekday'] = pd.to_datetime(agg['date']).dt.dayofweek.astype('int8')\n",
    "\n",
    "weather_data = {\n",
    "    'date': pd.date_range(\"2024-08-01\", \"2024-08-31\", freq='D').normalize(),\n",
    "    'temp_high_F': [95, 88, 92, 88, 87, 78, 69, 71, 85, 89, 84, 79, 82, 79, 78, 72, 71, 69, 75, 70, 76, 76, 83, 84, 81, 77, 74, 91, 69, 70, 79],\n",
    "    'temp_low_F':  [74, 72, 74, 70, 72, 62, 59, 60, 64, 74, 69, 65, 64, 66, 65, 64, 63, 65, 65, 60, 58, 58, 60, 62, 64, 65, 63, 65, 62, 58, 62],\n",
    "    'precip_in':   [0.00,0.77,0.00,0.36,0.00,0.58,0.24,0.22,0.31,0.12,0.00,0.00,0.00,0.00,0.89,0.00,0.00,0.03,0.11,0.01,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00]\n",
    "}\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "agg = agg.merge(weather_df, on='date', how='left')\n",
    "\n",
    "agg['start_station_id'] = agg['start_station_id'].astype('category')\n",
    "\n",
    "# --- Features und Target ---\n",
    "X = pd.DataFrame({\n",
    "    'station_code': agg['start_station_id'].cat.codes,\n",
    "    'hour': agg['hour'],\n",
    "    'weekday': agg['weekday'],\n",
    "    'temp_high_F': agg['temp_high_F'],\n",
    "    'temp_low_F': agg['temp_low_F'],\n",
    "    'precip_in': agg['precip_in']\n",
    "})\n",
    "y = agg['count']\n",
    "\n",
    "# --- Modelltraining auf dem gesamten Datensatz ---\n",
    "rf = RandomForestRegressor(n_jobs=-1, max_depth=10, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# --- Vorhersagen f√ºr alle Daten ---\n",
    "agg['prediction'] = rf.predict(X).round(1)\n",
    "\n",
    "# --- Sortieren nach Datum ---\n",
    "agg_sorted = agg.sort_values(by='date').reset_index(drop=True)\n",
    "\n",
    "# Nur gew√ºnschte Spalten\n",
    "agg_display = agg_sorted[['date', 'start_station_id', 'prediction']]\n",
    "\n",
    "# Ausgabe in Chunks (z.B. 5000 Zeilen pro Chunk)\n",
    "chunk_size = 5000\n",
    "for i in range(0, len(agg_display), chunk_size):\n",
    "    display(HTML(agg_display.iloc[i:i+chunk_size].to_html(index=False)))\n",
    "\n",
    "# Optional: kompletten Datensatz speichern\n",
    "agg.to_csv(\"bike_predictions_all_data_with_full_predictions.csv\", index=False)\n",
    "print(\"‚úÖ Datei 'bike_predictions_all_data_with_full_predictions.csv' gespeichert.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# --- Funktion zum Einlesen und Aggregieren einzelner CSV-Dateien ---\n",
    "def load_and_prepare(filepath):\n",
    "    print(f\"üìÇ Lade Datei: {filepath}\")\n",
    "    df = pd.read_csv(filepath, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "    # Spaltennamen vereinheitlichen\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    if 'started_at' not in df.columns:\n",
    "        raise ValueError(f\"‚ùå Die Datei '{filepath}' enth√§lt keine Spalte 'started_at'. Gefundene Spalten: {list(df.columns)}\")\n",
    "\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], dayfirst=True, errors='coerce')\n",
    "    df = df.dropna(subset=['started_at'])\n",
    "    df['date'] = df['started_at'].dt.normalize()\n",
    "    df['hour'] = df['started_at'].dt.hour.astype('int8')\n",
    "    df['weekday'] = df['started_at'].dt.dayofweek.astype('int8')\n",
    "    agg = df.groupby(['start_station_id', 'date', 'hour']).size().reset_index(name='count')\n",
    "    agg['weekday'] = pd.to_datetime(agg['date']).dt.dayofweek.astype('int8')\n",
    "    return agg\n",
    "\n",
    "# --- Alle drei Monatsdateien einlesen ---\n",
    "agg_2022 = load_and_prepare(\"202208-bluebikes-tripdata.csv\")\n",
    "agg_2023 = load_and_prepare(\"202308-bluebikes-tripdata.csv\")\n",
    "agg_2024 = load_and_prepare(\"202408-bluebikes-tripdata.csv\")\n",
    "\n",
    "# --- Zusammenf√ºhren ---\n",
    "agg_all = pd.concat([agg_2022, agg_2023, agg_2024], ignore_index=True)\n",
    "\n",
    "# --- Wetterdaten f√ºr August 2024 ---\n",
    "weather_data = {\n",
    "    'date': pd.date_range(\"2024-08-01\", \"2024-08-31\", freq='D').normalize(),\n",
    "    'temp_high_F': [95, 88, 92, 88, 87, 78, 69, 71, 85, 89, 84, 79, 82, 79, 78, 72, 71, 69, 75, 70, 76, 76, 83, 84, 81, 77, 74, 91, 69, 70, 79],\n",
    "    'temp_low_F':  [74, 72, 74, 70, 72, 62, 59, 60, 64, 74, 69, 65, 64, 66, 65, 64, 63, 65, 65, 60, 58, 58, 60, 62, 64, 65, 63, 65, 62, 58, 62],\n",
    "    'precip_in':   [0.00,0.77,0.00,0.36,0.00,0.58,0.24,0.22,0.31,0.12,0.00,0.00,0.00,0.00,0.89,0.00,0.00,0.03,0.11,0.01,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00]\n",
    "}\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "\n",
    "# --- Wetterdaten zuordnen ---\n",
    "agg_all = agg_all.merge(weather_df, on='date', how='left')\n",
    "agg_all['temp_high_F'] = agg_all['temp_high_F'].fillna(weather_df['temp_high_F'].mean())\n",
    "agg_all['temp_low_F'] = agg_all['temp_low_F'].fillna(weather_df['temp_low_F'].mean())\n",
    "agg_all['precip_in'] = agg_all['precip_in'].fillna(0)\n",
    "\n",
    "agg_all['start_station_id'] = agg_all['start_station_id'].astype('category')\n",
    "\n",
    "# --- Features & Target ---\n",
    "X = pd.DataFrame({\n",
    "    'station_code': agg_all['start_station_id'].cat.codes,\n",
    "    'hour': agg_all['hour'],\n",
    "    'weekday': agg_all['weekday'],\n",
    "    'temp_high_F': agg_all['temp_high_F'],\n",
    "    'temp_low_F': agg_all['temp_low_F'],\n",
    "    'precip_in': agg_all['precip_in']\n",
    "})\n",
    "y = agg_all['count']\n",
    "\n",
    "# --- Modelltraining ---\n",
    "rf = RandomForestRegressor(n_jobs=-1, max_depth=10, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# --- Interaktive Vorhersagefunktion ---\n",
    "def predict_count(date_str, hour, station_id):\n",
    "    date = pd.to_datetime(date_str).normalize()\n",
    "    weekday = date.dayofweek\n",
    "    weather_row = weather_df[weather_df['date'] == date]\n",
    "\n",
    "    if weather_row.empty:\n",
    "        temp_high = weather_df['temp_high_F'].mean()\n",
    "        temp_low = weather_df['temp_low_F'].mean()\n",
    "        precip = 0\n",
    "    else:\n",
    "        temp_high = weather_row['temp_high_F'].values[0]\n",
    "        temp_low = weather_row['temp_low_F'].values[0]\n",
    "        precip = weather_row['precip_in'].values[0]\n",
    "\n",
    "    if station_id not in agg_all['start_station_id'].unique():\n",
    "        return f\"‚ùå Station-ID {station_id} nicht im Datensatz enthalten.\"\n",
    "\n",
    "    station_code = agg_all['start_station_id'].cat.categories.get_loc(station_id)\n",
    "\n",
    "    input_features = pd.DataFrame([{\n",
    "        'station_code': station_code,\n",
    "        'hour': int(hour),\n",
    "        'weekday': int(weekday),\n",
    "        'temp_high_F': float(temp_high),\n",
    "        'temp_low_F': float(temp_low),\n",
    "        'precip_in': float(precip)\n",
    "    }])\n",
    "\n",
    "    prediction = rf.predict(input_features)[0]\n",
    "    return f\"üìà Vorhersage f√ºr {date_str} um {hour}:00 Uhr an Station {station_id}: {prediction:.1f} Fahrten\"\n",
    "\n",
    "# --- Beispielaufruf ---\n",
    "print(predict_count(\"2024-08-17\", 14, 31206.0))\n"
   ],
   "id": "f8e84b3f41581315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# üöÄ Automatische Harmonisierung\n",
    "def load_and_normalize(filepath):\n",
    "    print(f\"üì• Lade Datei: {filepath}\")\n",
    "    df = pd.read_csv(filepath, on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "    # üîß Spaltennamen vereinheitlichen\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "\n",
    "    # üß≠ M√∂gliche Umbenennungen ins Standardformat\n",
    "    col_rename_map = {\n",
    "        'starttime': 'started_at',\n",
    "        'stoptime': 'ended_at',\n",
    "        'start_station_id': 'start_station_id',\n",
    "        'start_station_name': 'start_station_name',\n",
    "        'end_station_id': 'end_station_id',\n",
    "        'end_station_name': 'end_station_name',\n",
    "        'usertype': 'member_casual',\n",
    "        'bikeid': 'rideable_type'  # dummy fallback\n",
    "    }\n",
    "\n",
    "    # üß† Wenn 'started_at' nicht da ist, aber z.‚ÄØB. 'starttime', dann umbenennen\n",
    "    for col_old, col_new in col_rename_map.items():\n",
    "        if col_old in df.columns and col_new not in df.columns:\n",
    "            df = df.rename(columns={col_old: col_new})\n",
    "\n",
    "    # ‚úÖ Vereinheitlichung: usertype ‚Üí member_casual\n",
    "    if 'member_casual' in df.columns:\n",
    "        df['member_casual'] = df['member_casual'].replace({\n",
    "            'Subscriber': 'member',\n",
    "            'Customer': 'casual'\n",
    "        })\n",
    "\n",
    "    # üïí Zeitfelder zu datetime, falls vorhanden\n",
    "    if 'started_at' in df.columns:\n",
    "        df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "        df['date'] = df['started_at'].dt.date\n",
    "        df['hour'] = df['started_at'].dt.hour\n",
    "        df['weekday'] = df['started_at'].dt.dayofweek\n",
    "\n",
    "    return df\n",
    "\n",
    "# üîÑ Alle relevanten Dateien laden\n",
    "files = [\"202208-bluebikes-tripdata.csv\", \"202308-bluebikes-tripdata.csv\", \"202408-bluebikes-tripdata.csv\"]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        df = load_and_normalize(file)\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Datei nicht gefunden: {file}\")\n",
    "\n",
    "# üìä Zusammenf√ºgen\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"‚úÖ Alle Dateien wurden erfolgreich vereinheitlicht und zusammengef√ºhrt.\")\n",
    "\n",
    "# ‚úÖ Beispiel f√ºr Weiterverarbeitung\n",
    "# df_all.to_csv(\"bluebikes_all_normalized.csv\", index=False)\n"
   ],
   "id": "d418fa2efdac8ece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# --- Punkt f√ºr Boston (Koordinaten) ---\n",
    "boston = Point(42.3601, -71.0589)\n",
    "\n",
    "# --- Funktion zur Wetterabfrage f√ºr einen bestimmten August ---\n",
    "def fetch_august_weather(year):\n",
    "    start = datetime(year, 8, 1)   # <-- KEIN tzinfo mehr\n",
    "    end   = datetime(year, 8, 31)\n",
    "\n",
    "    data = Daily(boston, start, end)\n",
    "    data = data.fetch().reset_index()\n",
    "\n",
    "    # Umrechnungen: Celsius ‚Üí Fahrenheit, mm ‚Üí inch\n",
    "    data['temp_high_F'] = data['tmax'] * 9 / 5 + 32\n",
    "    data['temp_low_F']  = data['tmin'] * 9 / 5 + 32\n",
    "    data['precip_in']   = data['prcp'] / 25.4\n",
    "\n",
    "    # Relevante Spalten ausw√§hlen\n",
    "    return data[['time', 'temp_high_F', 'temp_low_F', 'precip_in']].rename(columns={'time': 'date'})\n",
    "\n",
    "# --- Wetterdaten f√ºr 2022 und 2023 abrufen ---\n",
    "weather_2022 = fetch_august_weather(2022)\n",
    "weather_2023 = fetch_august_weather(2023)\n",
    "\n",
    "# --- Zusammenf√ºhren ---\n",
    "weather_df = pd.concat([weather_2022, weather_2023], ignore_index=True)\n",
    "\n",
    "# --- Ausgabe pr√ºfen ---\n",
    "print(weather_df)\n"
   ],
   "id": "80f8e0b6f94aa051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_bike_data(filenames):\n",
    "    dfs = []\n",
    "\n",
    "    for f in filenames:\n",
    "        print(f\"Lade Datei: {f}\")\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # Versuche alle m√∂glichen Namen f√ºr Zeitspalte\n",
    "        time_col = None\n",
    "        for candidate in ['started_at', 'start_time', 'starttime', 'start']:\n",
    "            if candidate in df.columns:\n",
    "                time_col = candidate\n",
    "                break\n",
    "        if not time_col:\n",
    "            raise KeyError(f\"Keine g√ºltige Zeitspalte in Datei {f}\")\n",
    "\n",
    "        df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "        df = df.dropna(subset=[time_col])\n",
    "        df['started_at'] = df[time_col]\n",
    "\n",
    "        # Versuche alle m√∂glichen Namen f√ºr Start-Station\n",
    "        station_col = None\n",
    "        for candidate in ['start_station_name', 'start station name', 'start_station', 'start location']:\n",
    "            if candidate in df.columns:\n",
    "                station_col = candidate\n",
    "                break\n",
    "        if not station_col:\n",
    "            raise KeyError(f\"Keine g√ºltige Startstations-Spalte in Datei {f}\")\n",
    "\n",
    "        df = df.dropna(subset=[station_col])\n",
    "        df['start_station_name'] = df[station_col]\n",
    "\n",
    "        # Features erstellen\n",
    "        df['date'] = df['started_at'].dt.date\n",
    "        df['hour'] = df['started_at'].dt.hour\n",
    "\n",
    "        # Aggregation\n",
    "        df_agg = (\n",
    "            df.groupby(['date', 'hour', 'start_station_name'])\n",
    "            .size()\n",
    "            .reset_index(name='trip_count')\n",
    "        )\n",
    "        dfs.append(df_agg)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n"
   ],
   "id": "e70fd2f52c333337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "2d10614ac2c32a6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Liste der Datendateien (hier: August 2022, 2023, 2024)\n",
    "files = [\"202208-bluebikes-tripdata.csv\", \"202308-bluebikes-tripdata.csv\", \"202408-bluebikes-tripdata.csv\"]\n",
    "\n",
    "df_list = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "    # Spalten bereinigen (evtl. Umbenennung, Kleinschreibung, Entfernen von Leerzeichen)\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "    # Zeits√§ule vereinheitlichen\n",
    "    if 'started_at' in df.columns:\n",
    "        time_col = 'started_at'\n",
    "    elif 'starttime' in df.columns:\n",
    "        time_col = 'starttime'\n",
    "    else:\n",
    "        raise KeyError(f\"Keine g√ºltige Zeit-Spalte in {file}\")\n",
    "    # Datum konvertieren (Tag zuerst, falls z.B. europ. Datumsformat)\n",
    "    df[time_col] = pd.to_datetime(df[time_col], dayfirst=True, errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])  # ung√ºltige Zeitstempel entfernen\n",
    "    df['date'] = df[time_col].dt.date\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "# Alle Daten zusammenf√ºhren\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n"
   ],
   "id": "47918334dd73b6a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Anzahl der Fahrten pro Datum (Jahr-Monat-Tag) ermitteln\n",
    "daily_counts = df_all.groupby('date').size().reset_index(name='rides')\n",
    "\n",
    "# Datumsspalte als datetime-Objekt (optional)\n",
    "daily_counts['date'] = pd.to_datetime(daily_counts['date'])\n"
   ],
   "id": "3a3f4bd145bf7824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from meteostat import Point, Daily\n",
    "from datetime import datetime\n",
    "\n",
    "# Wetterdaten f√ºr Boston (Koordinaten)\n",
    "boston = Point(42.3601, -71.0589)\n",
    "\n",
    "def fetch_august_weather(year):\n",
    "    start = datetime(year, 8, 1)\n",
    "    end   = datetime(year, 8, 31)\n",
    "    data = Daily(boston, start, end)\n",
    "    data = data.fetch().reset_index()\n",
    "    # Konvertierungen: Celsius ‚Üí Fahrenheit, mm ‚Üí Zoll\n",
    "    data['temp_high_F'] = data['tmax'] * 9/5 + 32\n",
    "    data['temp_low_F']  = data['tmin'] * 9/5 + 32\n",
    "    data['precip_in']   = data['prcp'] / 25.4\n",
    "    return data[['time', 'temp_high_F', 'temp_low_F', 'precip_in']].rename(columns={'time': 'date'})\n",
    "\n",
    "# Wetter f√ºr August 2022 und 2023 abrufen und zusammenf√ºhren\n",
    "weather_2022 = fetch_august_weather(2022)\n",
    "weather_2023 = fetch_august_weather(2023)\n",
    "weather_df = pd.concat([weather_2022, weather_2023], ignore_index=True)\n"
   ],
   "id": "4cc487d29a68f89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Datum in daily_counts bereits datetime, gleiche Basis wie weather_df\n",
    "daily_counts = daily_counts.merge(weather_df, on='date', how='left')\n",
    "\n",
    "# Fehlende Wetterwerte (z.B. f√ºr 2024 oder Tage ohne Daten) mit Mittelwerten auff√ºllen\n",
    "daily_counts['temp_high_F'] = daily_counts['temp_high_F'].fillna(weather_df['temp_high_F'].mean())\n",
    "daily_counts['temp_low_F']  = daily_counts['temp_low_F'].fillna(weather_df['temp_low_F'].mean())\n",
    "daily_counts['precip_in']   = daily_counts['precip_in'].fillna(0)\n",
    "\n",
    "# Weitere Merkmale extrahieren\n",
    "daily_counts['year']    = daily_counts['date'].dt.year\n",
    "daily_counts['month']   = daily_counts['date'].dt.month\n",
    "daily_counts['weekday'] = daily_counts['date'].dt.dayofweek  # Montag=0, Sonntag=6\n"
   ],
   "id": "79ba1032b75b744a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Feature-Matrix und Zielwert\n",
    "X = daily_counts[['year','month','weekday','temp_high_F','temp_low_F','precip_in']]\n",
    "y = daily_counts['rides']\n",
    "\n",
    "# Optional: Testdaten f√ºr Evaluierung abtrennen\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelltraining\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersage auf Validierungsset\n",
    "y_pred = rf.predict(X_val)\n",
    "rise = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"RMSE auf Validierungsdaten: {rise:.1f} Fahrten\")\n"
   ],
   "id": "dc3d01239cf27af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Endg√ºltiges Modell auf allen Daten trainieren\n",
    "rf_final = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_final.fit(X, y)\n"
   ],
   "id": "23ab5942b86e2366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def predict_rides(date_str):\n",
    "    \"\"\"\n",
    "    Gibt die prognostizierte Anzahl der Fahrten f√ºr ein gegebenes Datum zur√ºck.\n",
    "    date_str: Datum als String im Format \"YYYY-MM-DD\"\n",
    "    \"\"\"\n",
    "    # Datum verarbeiten\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    weekday = date.weekday()\n",
    "\n",
    "    # Wetterdaten f√ºr das gegebene Datum abrufen (aus weather_df, das 2022+2023 enth√§lt)\n",
    "    date_pd = pd.to_datetime(date)\n",
    "    row = weather_df[weather_df['date'] == date_pd]\n",
    "    if not row.empty:\n",
    "        temp_high = float(row['temp_high_F'])\n",
    "        temp_low  = float(row['temp_low_F'])\n",
    "        precip    = float(row['precip_in'])\n",
    "    else:\n",
    "        # Falls keine Wetterdaten vorhanden (z.B. k√ºnftiges Datum), mit Durchschnittswerten arbeiten\n",
    "        temp_high = weather_df['temp_high_F'].mean()\n",
    "        temp_low  = weather_df['temp_low_F'].mean()\n",
    "        precip    = 0.0\n",
    "\n",
    "    # Feature-Datenframe f√ºr die Vorhersage erstellen\n",
    "    features = pd.DataFrame([{\n",
    "        'year': year,\n",
    "        'month': month,\n",
    "        'weekday': weekday,\n",
    "        'temp_high_F': temp_high,\n",
    "        'temp_low_F': temp_low,\n",
    "        'precip_in': precip\n",
    "    }])\n",
    "\n",
    "    # Vorhersage mit dem trainierten Random Forest\n",
    "    prediction = rf_final.predict(features)[0]\n",
    "    return prediction\n",
    "\n",
    "# Beispiel: Vorhersage f√ºr den 1. August 2024 um 00:00 Uhr\n",
    "date_input = \"2024-08-5\"\n",
    "predicted_rides = predict_rides(date_input)\n",
    "print(f\"Vorhergesagte Anzahl der Fahrten am {date_input}: {predicted_rides:.0f}\")\n"
   ],
   "id": "6ba77e2b1da21235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f408a8ab443a2ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv_safe(filepath, parse_dates=None):\n",
    "    try:\n",
    "        # Versuche mit Komma als Trennzeichen\n",
    "        df = pd.read_csv(filepath, sep=';' , parse_dates=parse_dates)\n",
    "        if 'started_at' not in df.columns or 'ended_at' not in df.columns:\n",
    "            raise ValueError(\"Spalten fehlen ‚Äì versuche Semikolon\")\n",
    "    except (ValueError, pd.errors.ParserError):\n",
    "        # Fallback: Semikolon als Trennzeichen\n",
    "        df = pd.read_csv(filepath, delimiter=';', parse_dates=parse_dates)\n",
    "    return df\n",
    "\n",
    "def load_2024_data(filepath):\n",
    "    df = load_csv_safe(filepath,  parse_dates=['started_at', 'ended_at'])\n",
    "    df_clean = df[['ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
    "                   'start_station_name', 'start_station_id',\n",
    "                   'end_station_name', 'end_station_id',\n",
    "                   'start_lat', 'start_lng', 'end_lat', 'end_lng',\n",
    "                   'member_casual']].copy()\n",
    "    df_clean.rename(columns={'member_casual': 'user_type'}, inplace=True)\n",
    "    return df_clean\n",
    "\n",
    "def load_2023_data(filepath):\n",
    "    df1 = load_csv_safe(filepath, parse_dates=['started_at', 'ended_at'])\n",
    "    df_clean = df1[['ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
    "                   'start_station_name', 'start_station_id',\n",
    "                   'end_station_name', 'end_station_id',\n",
    "                   'start_lat', 'start_lng', 'end_lat', 'end_lng',\n",
    "                   'member_casual']].copy()\n",
    "    df_clean.rename(columns={'member_casual': 'user_type'}, inplace=True)\n",
    "    return df_clean\n",
    "\n",
    "def load_2022_data(filepath):\n",
    "    df2 = load_csv_safe(filepath, parse_dates=['started_at', 'ended_at'])\n",
    "    df_clean = df2[['tripduration', 'started_at', 'ended_at',\n",
    "                   'start station name', 'start station id',\n",
    "                   'end station name', 'end station id',\n",
    "                   'start station latitude', 'start station longitude',\n",
    "                   'end station latitude', 'end station longitude',\n",
    "                   'bikeid', 'usertype']].copy()\n",
    "    df_clean.rename(columns={\n",
    "        'start station name': 'start_station_name',\n",
    "        'start station id': 'start_station_id',\n",
    "        'end station name': 'end_station_name',\n",
    "        'end station id': 'end_station_id',\n",
    "        'start station latitude': 'start_lat',\n",
    "        'start station longitude': 'start_lng',\n",
    "        'end station latitude': 'end_lat',\n",
    "        'end station longitude': 'end_lng',\n",
    "        'usertype': 'user_type'\n",
    "    }, inplace=True)\n",
    "    return df_clean\n",
    "\n",
    "# --- DATEN LADEN ---\n",
    "\n",
    "df2024 = load_2024_data(\"202408-bluebikes-tripdata.csv\")\n",
    "df2023 = load_2023_data(\"202308-bluebikes-tripdata.csv\")\n",
    "df2022 = load_2022_data(\"202208-bluebikes-tripdata.csv\")\n",
    "\n",
    "# Gemeinsame Spalten erzwingen\n",
    "common_cols = list(set(df2024.columns) & set(df2023.columns) & set(df2022.columns))\n",
    "df_all = pd.concat([df2024[common_cols], df2023[common_cols], df2022[common_cols]], ignore_index=True)\n",
    "\n",
    "print(\"Daten erfolgreich geladen.\")\n",
    "print(df_all.head())\n"
   ],
   "id": "40b5c35c86cd6c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "297f6f8994bec1d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ---------------------\n",
    "# 1. CSV-Dateien laden\n",
    "# ---------------------\n",
    "\n",
    "df2022 = pd.read_csv('202208-bluebikes-tripdata.csv', sep=';', parse_dates=['started_at', 'ended_at'])\n",
    "df2023 = pd.read_csv('202308-bluebikes-tripdata.csv', sep=';', parse_dates=['started_at', 'ended_at'])\n",
    "df2024 = pd.read_csv('202408-bluebikes-tripdata.csv', sep=';', parse_dates=['started_at', 'ended_at'])\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Spalten vereinheitlichen\n",
    "# ----------------------------\n",
    "\n",
    "# 2022 hat andere Spaltennamen ‚Üí umbenennen\n",
    "df2022 = df2022.rename(columns={\n",
    "    'start station id': 'start_station_id',\n",
    "    'start station name': 'start_station_name',\n",
    "    'end station id': 'end_station_id',\n",
    "    'end station name': 'end_station_name',\n",
    "    'start station latitude': 'start_lat',\n",
    "    'start station longitude': 'start_lng',\n",
    "    'end station latitude': 'end_lat',\n",
    "    'end station longitude': 'end_lng',\n",
    "    'tripduration': 'trip_duration',\n",
    "    'bikeid': 'ride_id',\n",
    "    'usertype': 'member_casual'\n",
    "})\n",
    "\n",
    "# 2023 und 2024 haben dieselbe Struktur ‚Üí kein Rename n√∂tig\n",
    "\n",
    "# Sicherstellen, dass alle DataFrames dieselben Spalten haben (ggf. unn√∂tige Spalten entfernen)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Alle Jahre zu einem Dataset\n",
    "# -------------------------------\n",
    "\n",
    "data = pd.concat([df2022, df2023, df2024], ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "f732cb10fbaa80cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Standort K√∂ln (Latitude, Longitude, H√∂henmeter, optional)\n",
    "koeln = Point(50.9375, 6.9603, 100)\n",
    "\n",
    "# Zeitraum: Vom fr√ºhesten bis sp√§testen Datum aus den CSV-Daten\n",
    "start = data['date'].min()\n",
    "end   = data['date'].max()\n",
    "\n",
    "# Abrufen der t√§glichen Wetterdaten (2020 als Beispieljahr)\n",
    "weather = Daily(koeln, start, end).fetch()\n",
    "\n",
    "# Auswahl typischer Wettermerkmale (tavg=¬∞C, prcp=mm, wspd=Windgeschw.)\n",
    "data = data.merge(weather[['tavg', 'prcp', 'wspd']], on='date', how='left')\n"
   ],
   "id": "341230883f6aafc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Installation der notwendigen Pakete (ausf√ºhrbar in einer Jupyter-Umgebung)\n",
    "# Falls Du lokal arbeitest, bitte diese Zeile in Deiner Shell ausf√ºhren:\n",
    "# pip install pandas numpy scikit-learn meteostat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from meteostat import Daily, Stations\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Schritt 1: CSV-Dateien einlesen (Anpassung der Spaltennamen je nach Jahr)\n",
    "df_2022 = pd.read_csv(\"202208-bluebikes-tripdata.csv\",  sep=';' ,parse_dates=['started_at', 'ended_at'])\n",
    "df_2023 = pd.read_csv(\"202308-bluebikes-tripdata.csv\",  sep=';' ,parse_dates=['started_at', 'ended_at'])\n",
    "df_2024 = pd.read_csv(\"202408-bluebikes-tripdata.csv\",  sep=';' ,parse_dates=['started_at', 'ended_at'])\n",
    "\n",
    "# Spalten vereinheitlichen\n",
    "df_2022.rename(columns={\n",
    "    'start station id': 'start_station_id',\n",
    "    'start station name': 'start_station_name',\n",
    "    'start station latitude': 'start_lat',\n",
    "    'start station longitude': 'start_lng',\n",
    "    'end station id': 'end_station_id',\n",
    "    'end station name': 'end_station_name',\n",
    "    'end station latitude': 'end_lat',\n",
    "    'end station longitude': 'end_lng'\n",
    "}, inplace=True)\n",
    "for df in [df_2022, df_2023, df_2024]:\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df.dropna(subset=['started_at', 'ended_at'], inplace=True)\n",
    "    df = df[df['ended_at'] > df['started_at']]\n",
    "    df['duration_min'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "\n",
    "# Einheitliche Dauerberechnung\n",
    "df_2022['duration_min'] = (df_2022['ended_at'] - df_2022['started_at']).dt.total_seconds() / 60\n",
    "df_2023['duration_min'] = (df_2023['ended_at'] - df_2023['started_at']).dt.total_seconds() / 60\n",
    "df_2024['duration_min'] = (df_2024['ended_at'] - df_2024['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "\n",
    "# Alle zusammenf√ºhren\n",
    "df_all = pd.concat([df_2022, df_2023, df_2024], ignore_index=True)\n",
    "\n",
    "# Neue Spalten\n",
    "df_all['date'] = df_all['started_at'].dt.date\n",
    "df_all['hour'] = df_all['started_at'].dt.hour\n",
    "df_all['weekday'] = df_all['started_at'].dt.weekday\n",
    "\n",
    "# Gruppierung\n",
    "agg = df_all.groupby(['start_station_id', 'date', 'hour']).size().reset_index(name='count')\n",
    "\n",
    "# Top 20 Stationen\n",
    "top_stations = agg.groupby('start_station_id')['count'].sum().sort_values(ascending=False).head(20).index\n",
    "agg = agg[agg['start_station_id'].isin(top_stations)]\n",
    "\n",
    "# Schritt 2: Wetterdaten Boston Logan Airport (72509)\n",
    "start = datetime(2022, 8, 1)\n",
    "end = datetime(2024, 8, 31)\n",
    "weather = Daily('72509', start, end)\n",
    "weather = weather.fetch().reset_index()\n",
    "weather['date'] = weather['time'].dt.date\n",
    "\n",
    "# Umrechnung ¬∞C -> ¬∞F und mm -> inch\n",
    "weather['tmin_F'] = weather['tmin'] * 9/5 + 32\n",
    "weather['tmax_F'] = weather['tmax'] * 9/5 + 32\n",
    "weather['prcp_in'] = weather['prcp'] / 25.4\n",
    "\n",
    "weather = weather[['date', 'tmin_F', 'tmax_F', 'prcp_in']]\n",
    "\n",
    "# Merge\n",
    "df_final = pd.merge(agg, weather, on='date', how='left')\n",
    "\n",
    "# Hinzuf√ºgen von weekday\n",
    "df_final['weekday'] = pd.to_datetime(df_final['date']).dt.weekday\n",
    "\n",
    "# Modellierung\n",
    "X = df_final[['start_station_id', 'hour', 'weekday', 'tmax_F', 'tmin_F', 'prcp_in']]\n",
    "y = df_final['count']\n",
    "\n",
    "# One-Hot-Encoding f√ºr start_station_id\n",
    "X = pd.get_dummies(X, columns=['start_station_id'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# In NumPy-Arrays umwandeln und flach machen\n",
    "y_test = np.array(y_test).astype(float).ravel()\n",
    "y_pred = np.array(y_pred).astype(float).ravel()\n",
    "\n",
    "# Pr√ºfen auf NaN oder inf\n",
    "if np.isnan(y_test).any() or np.isnan(y_pred).any():\n",
    "    raise ValueError(\"y_test oder y_pred enth√§lt NaN-Werte\")\n",
    "if np.isinf(y_test).any() or np.isinf(y_pred).any():\n",
    "    raise ValueError(\"y_test oder y_pred enth√§lt Inf-Werte\")\n",
    "print(\"Typ y_test:\", type(y_test))\n",
    "print(\"Typ y_pred:\", type(y_pred))\n",
    "print(\"Shape y_test:\", np.shape(y_test))\n",
    "print(\"Shape y_pred:\", np.shape(y_pred))\n",
    "print(\"Erste Werte y_test:\", y_test[:5])\n",
    "print(\"Erste Werte y_pred:\", y_pred[:5])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rmse_manual(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "rmse = rmse_manual(y_test, y_pred)\n",
    "print(\"Manueller RMSE:\", rmse)\n",
    "\n"
   ],
   "id": "17b28c5425ea6327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Installation der notwendigen Pakete (ausf√ºhrbar in einer Jupyter-Umgebung)\n",
    "# Falls Du lokal arbeitest, bitte diese Zeile in Deiner Shell ausf√ºhren:\n",
    "# pip install pandas numpy scikit-learn meteostat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from meteostat import Daily, Stations\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Schritt 1: CSV-Dateien einlesen (Anpassung der Spaltennamen je nach Jahr)\n",
    "df_2022 = pd.read_csv(\"202208-bluebikes-tripdata.csv\",  sep=';' ,parse_dates=['started_at', 'ended_at'])\n",
    "df_2023 = pd.read_csv(\"202308-bluebikes-tripdata.csv\",  sep=';' ,parse_dates=['started_at', 'ended_at'])\n",
    "df_2024 = pd.read_csv(\"202408-bluebikes-tripdata.csv\",  sep=';' ,parse_dates=['started_at', 'ended_at'])\n",
    "\n",
    "# Spalten vereinheitlichen\n",
    "df_2022.rename(columns={\n",
    "    'start station id': 'start_station_id',\n",
    "    'start station name': 'start_station_name',\n",
    "    'start station latitude': 'start_lat',\n",
    "    'start station longitude': 'start_lng',\n",
    "    'end station id': 'end_station_id',\n",
    "    'end station name': 'end_station_name',\n",
    "    'end station latitude': 'end_lat',\n",
    "    'end station longitude': 'end_lng'\n",
    "}, inplace=True)\n",
    "for df in [df_2022, df_2023, df_2024]:\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df.dropna(subset=['started_at', 'ended_at'], inplace=True)\n",
    "    df = df[df['ended_at'] > df['started_at']]\n",
    "    df['duration_min'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "\n",
    "# Einheitliche Dauerberechnung\n",
    "df_2022['duration_min'] = (df_2022['ended_at'] - df_2022['started_at']).dt.total_seconds() / 60\n",
    "df_2023['duration_min'] = (df_2023['ended_at'] - df_2023['started_at']).dt.total_seconds() / 60\n",
    "df_2024['duration_min'] = (df_2024['ended_at'] - df_2024['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "\n",
    "# Alle zusammenf√ºhren\n",
    "df_all = pd.concat([df_2022, df_2023, df_2024], ignore_index=True)\n",
    "\n",
    "# Neue Spalten\n",
    "df_all['date'] = df_all['started_at'].dt.date\n",
    "df_all['hour'] = df_all['started_at'].dt.hour\n",
    "df_all['weekday'] = df_all['started_at'].dt.weekday\n",
    "\n",
    "# Gruppierung\n",
    "agg = df_all.groupby(['start_station_id', 'date', 'hour']).size().reset_index(name='count')\n",
    "\n",
    "# Top 20 Stationen\n",
    "top_stations = agg.groupby('start_station_id')['count'].sum().sort_values(ascending=False).head(20).index\n",
    "agg = agg[agg['start_station_id'].isin(top_stations)]\n",
    "\n",
    "# Schritt 2: Wetterdaten Boston Logan Airport (72509)\n",
    "start = datetime(2022, 8, 1)\n",
    "end = datetime(2024, 8, 31)\n",
    "weather = Daily('72509', start, end)\n",
    "weather = weather.fetch().reset_index()\n",
    "weather['date'] = weather['time'].dt.date\n",
    "\n",
    "# Umrechnung ¬∞C -> ¬∞F und mm -> inch\n",
    "weather['tmin_F'] = weather['tmin'] * 9/5 + 32\n",
    "weather['tmax_F'] = weather['tmax'] * 9/5 + 32\n",
    "weather['prcp_in'] = weather['prcp'] / 25.4\n",
    "\n",
    "weather = weather[['date', 'tmin_F', 'tmax_F', 'prcp_in']]\n",
    "\n",
    "# Merge\n",
    "df_final = pd.merge(agg, weather, on='date', how='left')\n",
    "\n",
    "# Hinzuf√ºgen von weekday\n",
    "df_final['weekday'] = pd.to_datetime(df_final['date']).dt.weekday\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "X = df_final[['start_station_id', 'hour', 'weekday', 'tmax_F', 'tmin_F', 'prcp_in']]\n",
    "y = df_final['count']\n",
    "\n",
    "# One-Hot-Encoding der Stationen\n",
    "X = pd.get_dummies(X, columns=['start_station_id'])\n",
    "\n",
    "# Trainings- und Testsplit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelltraining\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 4. Vorhersage-Funktion ---\n",
    "def predict_rides(model, station_id, date_str, hour, weather_df, feature_cols):\n",
    "    \"\"\"\n",
    "    Prognose der Anzahl der Fahrten f√ºr eine bestimmte Station, Datum und Stunde.\n",
    "\n",
    "    Args:\n",
    "        model: Trainiertes RandomForestRegressor-Modell.\n",
    "        station_id (int oder str): Startstation-ID.\n",
    "        date_str (str): Datum im Format 'YYYY-MM-DD'.\n",
    "        hour (int): Stunde des Tages (0-23).\n",
    "        weather_df (pd.DataFrame): Wetterdaten mit Spalten ['date', 'tmax_F', 'tmin_F', 'prcp_in'].\n",
    "        feature_cols (pd.Index): Feature-Spalten des trainierten Modells (One-Hot-Kodierung).\n",
    "\n",
    "    Returns:\n",
    "        float: Vorhergesagte Anzahl der Fahrten.\n",
    "    \"\"\"\n",
    "    date = pd.to_datetime(date_str).date()\n",
    "    weekday = pd.to_datetime(date_str).weekday()\n",
    "\n",
    "    # Wetterdaten f√ºr das Datum holen\n",
    "    weather_row = weather_df[weather_df['date'] == date]\n",
    "    if weather_row.empty:\n",
    "        raise ValueError(f\"Wetterdaten f√ºr {date} nicht gefunden.\")\n",
    "\n",
    "    tmax_F = weather_row['tmax_F'].values[0]\n",
    "    tmin_F = weather_row['tmin_F'].values[0]\n",
    "    prcp_in = weather_row['prcp_in'].values[0]\n",
    "\n",
    "    # Feature-Vektor initialisieren\n",
    "    data = {\n",
    "        'hour': hour,\n",
    "        'weekday': weekday,\n",
    "        'tmax_F': tmax_F,\n",
    "        'tmin_F': tmin_F,\n",
    "        'prcp_in': prcp_in\n",
    "    }\n",
    "\n",
    "    # One-Hot-Encoding der Stationen √ºbernehmen\n",
    "    for col in feature_cols:\n",
    "        if col.startswith('start_station_id_'):\n",
    "            data[col] = 1 if col == f'start_station_id_{station_id}' else 0\n",
    "\n",
    "    # DataFrame f√ºr Modellinput\n",
    "    X_pred = pd.DataFrame([data], columns=feature_cols)\n",
    "\n",
    "    # Vorhersage\n",
    "    prediction = model.predict(X_pred)[0]\n",
    "    return prediction\n",
    "\n",
    "# --- 5. Beispielvorhersage ---\n",
    "feature_cols = X.columns\n",
    "example_station = top_stations[0]  # z.B. erste Top-Station\n",
    "example_date = '2024-08-15'\n",
    "example_hour = 14\n",
    "\n",
    "predicted_count = predict_rides(model, example_station, example_date, example_hour, weather, feature_cols)\n",
    "print(f\"Vorhergesagte Anzahl Fahrten am {example_date} um {example_hour} Uhr an Station {example_station}: {predicted_count:.2f}\")\n"
   ],
   "id": "f94d7f83fadb415",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
